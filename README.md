# Handmade Custom GPT Model  

This notebook implements and partially trains a custom Generative Pre-Trained Transformer (GPT) model, minimizing the use of premade transformer objects or classes. A GPT is a powerful model capable of generating text by predicting the next token in a sequence based on the given context.  

---

## Key Components:  
- **Attention Mechanism**: The core feature that allows the model to focus on relevant parts of the input sequence.  

- **Positional Encoding**: A method to provide positional information to the model for sequential data.  

- **Transformer Architecture**: The design and logic behind the structure of the transformer model.  

- **Tokenization**: Converting text into a format that can be processed by the model.  

- **Data Preprocessing**: Preparing input data for training.  

- **Training Loop**: Updating the model's parameters to minimize the loss function.  

- **Evaluation Metrics**: Assessing the model's performance using appropriate metrics.  

- **Text Generation**: Generating text using the trained model.  

This implementation emphasizes the fundamental concepts of transformer architecture, aiming to provide a deeper understanding of the internal mechanisms of transformer-based models.  

---

## Project Goals:  
1. Build a custom GPT model from scratch with minimal reliance on premade transformer objects or classes.  

2. Gain a thorough understanding of the internal workings of transformer architectures.  

3. Train the model on text data for next-token prediction.  

4. Generate text using the custom-built model.  

Although I could not fully train the model due to time constraints, I trained it long enough to demonstrate the output of a partially trained GPT model. This serves as a practical example of the capabilities and limitations of an undertrained transformer-based model.  

---

**Sample output after creating a medium sized model and achieving a loss around 6 using Cross Entropy:**

Generated text: Once upon a time to be the story of the world’s new world are taking a big price of this year. Here is our one of the world that was first look at least a new study of a big story that has shown to see it, a lot of the most of how that have done it will be a bit on the other forms of a major. No. For example from the past a good as a series for a good news conference just over what most people who were true. This particular long way. Even before the year’s in particular a little many different episodes to see an incredible life, to be the past several reasons over our third a massive to the team’s that time or a good time. My first time since I would like one of hard time, which, and we came the good part of a post on one of the very happy with a time, but on its entire new year, but there wasn’s that time. "The way to hold
